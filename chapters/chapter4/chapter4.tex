\chapter{Experiments and Results}
\label{experiment}

\section{Experiments}

Our experiments compared real executions with our Python prototype,
with the original \wrench simulator, and with our \wrench-cache
extension. Executions included single-threaded and multi-threaded
applications, accessing data on local and network file systems. We
used two applications: a synthetic one, created to evaluate the
simulation model, and a real one, representative of neuroimaging
data processing.

Experiments were run on a dedicated cluster at
Concordia University, with one login node, 9 compute nodes, and 4
storage nodes connected with a 25 Gbps network. Each
compute node had 2 $\times$ 16-core Intel(R) Xeon(R) Gold 6130 CPU
@ 2.10GHz, 250~GiB of RAM, 6 $\times$ SSDs of 450~GiB each with the XFS
file system, 378~GiB of tmpfs, 126~GiB of devtmpfs file system,
CentOS~8.1 and NFS version 4. We used the \texttt{atop}
and \texttt{collectl} tools to monitor and collect memory status
and disk throughput. We cleared the page
cache before each application run to ensure comparable
conditions.

\begin{table}[b]
    \centering
    \begin{tabularx}{0.8\columnwidth}{c>{\centering\arraybackslash}X}
    \toprule
        Input size (GB)  & CPU time (s)\\
    \midrule
        3      & 4.4 \\
        20  & 28 \\
        50  & 75 \\
        75  & 110 \\
        100  & 155 \\
    \bottomrule
    \end{tabularx}
    \caption{Synthetic application parameters}
    \label{table:cputime}
\end{table}
    
The synthetic application, implemented in C, consisted of three single-core,
sequential tasks where each task read the file produced by the
previous task, incremented every byte of this file to emulate real
processing, and wrote the resulting data to disk. Files were
numbered by ascending access times (File 1 was the file read by Task 1, etc).
 The anonymous memory used by the application
was released after each task, which we also simulated in the Python
prototype and in WRENCH-cache. As our focus was on I/O rather than compute, we measured
application task CPU times on a cluster node
(Table~\ref{table:cputime}), and used these durations in our
simulations. For the Python prototype, we injected CPU times
directly in the simulation. For \wrench and \wrench-cache, we
determined the corresponding number of flops on a 1~Gflops CPU
and used these values in the simulation. The simulated
platform and application are available at
commit \href{https://github.com/wrench-project/wrench/tree/ec6b43561b95977002258c0fe37a4ecad8f1d33f/examples/basic-examples/io-pagecache}{ec6b43561b}.

We used the synthetic application in three experiments. In the
first one (\textit{Exp~1}), we ran \emph{a single instance} of
the application on a single cluster node, with different input file
sizes (20~GB, 50~GB, 75~GB, 100~GB), and with all I/Os directed to
the same local disk.
In the second experiment (\textit{Exp~2}), we ran
\emph{concurrent} instances of the application on a single node,
all application instances operating on different files of size 3~GB
stored in the same local disk. We varied the number of concurrent
application instances from 1 to 32 since cluster nodes had 32 CPU
cores.
In the third experiment (\textit{Exp~3}), we used the same
configuration as the previous one, albeit reading and writing
on a 50-GiB NFS-mounted partition of a 450-GiB remote disk of
another compute node. As is commonly configured in HPC
environments to avoid data loss, there was no client write cache
and the server cache was configured as writethrough instead of
writeback. NFS client and server read caches were enabled. 
Therefore, all the writes happened at disk bandwidth, but
reads could benefit from cache hits.

The real application was a workflow of the Nighres
toolbox~\cite{huntenburg2018nighres}, implementing cortical
reconstruction from brain images in four steps: skull stripping,
tissue classification, region extraction, and cortical
reconstruction. Each step read files produced by the previous step,
and wrote files that were or were not read by the subsequent step.
More information on this application is available in the Nighres
documentation at
\url{https://nighres.readthedocs.io/en/latest/auto_examples/example_02_cortical_depth_estimation.html}.
The application is implemented as a Python script that calls Java
image-processing routines. We used Python 3.6, Java 8, and Nighres
1.3.0. We patched the application to remove lazy data loading and
data compression, which made CPU time difficult to separate from
I/O time, and to capture task CPU times to inject them in the
simulation. The patched code is available at
\url{https://github.com/dohoangdzung/nighres}.
\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
    \toprule
        \multicolumn{1}{c}{Workflow step}& Input size       & Output size      & CPU time\\
                               & (MB)             & (MB)             & (s)\\
    \midrule
       Skull stripping         &  295             & 393               & 137 \\
       Tissue classification   &  197              & 1376              & 614 \\
       Region extraction       &  1376             & 885              & 76 \\
       Cortical reconstruction &  393              & 786              & 272\\
    \bottomrule
    \end{tabular} 
    \caption{Nighres application parameters}
    \label{table:nighres_stats}
    \end{table}
    \begin{table}[b]
        \centering
        \begin{tabularx}{\columnwidth}{ll
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X}
        \toprule
            \multicolumn{2}{c}{Bandwidths}  & Cluster (real) & Python prototype & \wrench simulator\\
        \midrule
        \multirow{2}{*}{Memory}      & read  & 6860 & 4812 & 4812\\
                                     & write & 2764 & 4812 & 4812\\
        \multirow{2}{*}{Local disk}  & read  & 510  & 465  & 465\\
                                     & write & 420  & 465  & 465\\
        \multirow{2}{*}{Remote disk} & read  & 515  & -    & 445\\
                                     & write & 375  & -    & 445\\
        \multicolumn{2}{l}{Network}  & 3000  & -    & 3000\\
        \bottomrule
    \end{tabularx}
    \caption{Bandwidth benchmarks (MBps) and simulator configurations.
    The bandwidths used in the simulations were the average of the measured read and write bandwidths.
    Network accesses were not simulated in the Python prototype.}
    \label{table:benchmark}
\end{table}

We used the real application in the fourth experiment
(\textit{Exp~4}), run on a single cluster node 
using a single local disk. We processed data from
participant 0027430 in the dataset of the Max Planck Institute for
Human Cognitive and Brain Sciences available at
\url{http://dx.doi.org/10.15387/fcp_indi.corr.mpg1}, leading to the
parameters in Table~\ref{table:nighres_stats}.


To parameterize the simulators, we benchmarked the
memory, local disk, remote disk (NFS), and network bandwidths
(Table~\ref{table:benchmark}). Since \simgrid, and thus \wrench, currently only supports
symmetrical bandwidths, we use the mean of the read and write
bandwidth values in our experiments.

% Finally, because \wrench simulates applications base on network-communication,
% we use an infinite bandwidth to eliminate network latency for local I/Os.

\section{Results}

\subsection{Single-threaded execution (Exp 1)}

\begin{figure*}
    \centering
    \begin{subfigure}{\linewidth}
        \centering
           \includegraphics[width=\linewidth]{result/single/figures/single_errors.pdf}
           \vspace*{-0.7cm}
           \caption{Absolute relative simulation errors}
           \vspace*{0.5cm}
           \label{fig:single_error}
        \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        %    Gray shades represent task phases (read, compute and write).
        %    Lines represent memory usage along pipeline execution time.}
           \includegraphics[width=\linewidth]{result/single/figures/single_memprof.pdf}
           \vspace*{-0.7cm}
           \caption{Memory profiles}
           \vspace*{0.5cm}
           \label{fig:single_memprof}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
           \includegraphics[width=\linewidth]{result/single/figures/cached_files.pdf}
           \caption{Cache contents \emph{after} application I/O operations}
        %    \textcolor{red}{Update real results of 20GB}}
           \label{fig:single_cache}
    \end{subfigure}
    \caption{Single-threaded results (\textit{Exp 1})}
\end{figure*}

The page cache simulation model drastically reduced I/O simulation
errors in each application task (Figure~\ref{fig:single_error}). The first read was not impacted
as it only involved uncached data. Errors were reduced from an average
of 345\% in the original \wrench to 46\% in the Python prototype and
39\% in \wrench-cache. Unsurprisingly, the original \wrench simulator
significantly overestimated read and write times, due to the lack
of page cache simulation. Results with files of 50~GB and 75~GB
showed similar behaviors and are not reported for brevity.

\wrench simulation errors were substantially lower with 100~GB
files than with 20~GB files, due to the fact that part of the
100~GB file needed to be read and written to disk, the only storage
device in \wrench, as it did not fit in cache. Conversely,
simulation errors of the Python prototype and \wrench-cache were higher with
100~GB files than with 20~GB files, due to idiosyncrasies in the kernel
flushing and eviction strategies that could not be easily modeled.

Simulated memory profiles were highly consistent with the real ones
(Figure~\ref{fig:single_memprof}). With 20~GB files, memory profiles almost exactly matched the
real ones, although dirty data seemed to be flushing faster in real
life than in simulation, a behavior also
observed with 100~GB files. With 100~GB files, used memory reached
total memory during the first write, triggering dirty data
flushing, and droped back to cached memory when application tasks
released anonymous memory. Simulated cached memory was highly
consistent with real values, except toward the end of Read 3 where
it slightly increased in simulation but not in reality. This
occurred due to the fact that after Write 2, File 3 was only partially
cached in simulation whereas it was entirely cached in the real
system. In all cases, dirty data remained under the dirty ratio as
expected. The Python prototype and \wrench-cache exhibited nearly
identical memory profiles, which reinforces the confidence in our
implementations.

The content of the simulated memory cache was also highly
consistent with reality (Figure~\ref{fig:single_cache}). With 20~GB
files, the simulated cache content exactly matched reality, since
all files fitted in page cache. With 100~GB files, a slight
discrepancy was observed after Write 2, which explains the
simulation error previously mentioned in Read 3. In the real
execution indeed, File 3 was entirely cached after Write 2, whereas
in the simulated execution, only a part of it was cached. This was
due to the fact that the Linux kernel tends to not evict pages that
belong to files being currently written (File 3 in this case),
which we could not easily reproduce in our model.

\begin{figure*}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{result/multi/figures/multi_local.pdf}
    \end{subfigure}
    \caption{Concurrent results with 3~GB files (\textit{Exp 2})}
    \label{fig:multi_local}
\end{figure*}

\subsection{Concurrent applications (Exp 2)}

The page cache model notably reduced \wrench's simulation error
for concurrent applications executed with local I/Os
(Figure~\ref{fig:multi_local}). For reads, \wrench-cache
slightly overestimated runtime, due to the discrepancy between
simulated and real read bandwidths mentioned before. 
For writes, \wrench-cache
retrieved a plateau similar to the one observed in the real
execution, marking the limit beyond which the page cache was
saturated with dirty data and needed flushing.

\begin{figure}[b]
    \begin{subfigure}{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth]{result/nighres/figures/nighres_errors.pdf}
    \end{subfigure}
    \caption{Real application results (\textit{Exp 4})}
    \label{fig:nighres}
\end{figure}

\subsection{Remote storage (Exp 3)}

Page cache simulation importantly reduced simulation error
on NFS storage as well (Figure~\ref{fig:multi_nfs}). This
manifested only for reads, as the NFS server used writethrough rather than writeback cache.
Both \wrench and \wrench-cache
underestimated write times due to the discrepancy between
simulated and real bandwidths mentioned previously. For reads,
this discrepancy only impacted the results beyond 22
applications since before this threshold, most reads resulted in cache
hits.


\subsection{Real application (Exp 4)}
Similar to the synthetic application, simulation errors were
substantially reduced by the WRENCH-cache simulator compared to
WRENCH (Figure~\ref{fig:nighres}). On average, errors were reduced
from 337~\% in WRENCH to 47~\% in WRENCH-cache. 
The first read happened entirely from disk and was therefore 
very accurately simulated by both WRENCH and WRENCH-cache.

\begin{figure*}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{result/multi/figures/multi_nfs.pdf}
    \end{subfigure}
    \caption{NFS results with 3~GB files (\textit{Exp 3})}
    \label{fig:multi_nfs}
\end{figure*}

\begin{figure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{result/multi/figures/simulation_time.pdf}
    \end{subfigure}
    \caption{Simulation time comparison. \wrench-cache scales
    linearly with the number of concurrent applications, albeit
    with a higher overhead than \wrench.}
    \label{fig:multi_time}
\end{figure}

\subsection{Simulation time}

As is the case for \wrench, simulation time with \wrench-cache scales
linearly with the number of concurrent applications
(Figure~\ref{fig:multi_time}, p \textless $10^{-24}$). However, the page
cache model substantially increases simulation time by
application, as can be seen by comparing regression slopes in
Figure~\ref{fig:multi_time}. Interestingly, \wrench-cache is faster with 
NFS I/Os than with local I/Os, most likely due to the use of writethrough
cache in NFS, which bypasses flushing operations.