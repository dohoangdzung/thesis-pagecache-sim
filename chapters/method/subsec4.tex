\section{Experiments}

Our experiments compared real executions with our Python prototype,
with the original \wrench simulator, and with our \wrench-cache
extension. Executions included single-threaded and multi-threaded
applications, accessing data on local and network file systems. We
used two applications: a synthetic one, created to evaluate the
simulation model, and a real one, representative of neuroimaging
data processing.

Experiments were run on a dedicated cluster at
Concordia University, with one login node, 9 compute nodes, and 4
storage nodes connected with a 25 Gbps network. Each
compute node had 2 $\times$ 16-core Intel(R) Xeon(R) Gold 6130 CPU
@ 2.10GHz, 250~GiB of RAM, 6 $\times$ SSDs of 450~GiB each with the XFS
file system, 378~GiB of tmpfs, 126~GiB of devtmpfs file system,
CentOS~8.1 and NFS version 4. We used the \texttt{atop}
and \texttt{collectl} tools to monitor and collect memory status
and disk throughput. We cleared the page
cache before each application run to ensure comparable
conditions.

\begin{table}[b]
    \centering
    \begin{tabularx}{0.8\columnwidth}{c>{\centering\arraybackslash}X}
    \toprule
        Input size (GB)  & CPU time (s)\\
    \midrule
        3      & 4.4 \\
        20  & 28 \\
        50  & 75 \\
        75  & 110 \\
        100  & 155 \\
    \bottomrule
    \end{tabularx}
    \caption{Synthetic application parameters}
    \label{table:cputime}
\end{table}
    
The synthetic application, implemented in C, consisted of three single-core,
sequential tasks where each task read the file produced by the
previous task, incremented every byte of this file to emulate real
processing, and wrote the resulting data to disk. Files were
numbered by ascending access times (File 1 was the file read by Task 1, etc).
 The anonymous memory used by the application
was released after each task, which we also simulated in the Python
prototype and in WRENCH-cache. As our focus was on I/O rather than compute, we measured
application task CPU times on a cluster node
(Table~\ref{table:cputime}), and used these durations in our
simulations. For the Python prototype, we injected CPU times
directly in the simulation. For \wrench and \wrench-cache, we
determined the corresponding number of flops on a 1~Gflops CPU
and used these values in the simulation. The simulated
platform and application are available at
commit \href{https://github.com/wrench-project/wrench/tree/ec6b43561b95977002258c0fe37a4ecad8f1d33f/examples/basic-examples/io-pagecache}{ec6b43561b}.

We used the synthetic application in three experiments. In the
first one (\textit{Exp~1}), we ran \emph{a single instance} of
the application on a single cluster node, with different input file
sizes (20~GB, 50~GB, 75~GB, 100~GB), and with all I/Os directed to
the same local disk.
In the second experiment (\textit{Exp~2}), we ran
\emph{concurrent} instances of the application on a single node,
all application instances operating on different files of size 3~GB
stored in the same local disk. We varied the number of concurrent
application instances from 1 to 32 since cluster nodes had 32 CPU
cores.
In the third experiment (\textit{Exp~3}), we used the same
configuration as the previous one, albeit reading and writing
on a 50-GiB NFS-mounted partition of a 450-GiB remote disk of
another compute node. As is commonly configured in HPC
environments to avoid data loss, there was no client write cache
and the server cache was configured as writethrough instead of
writeback. NFS client and server read caches were enabled. 
Therefore, all the writes happened at disk bandwidth, but
reads could benefit from cache hits.

The real application was a workflow of the Nighres
toolbox~\cite{huntenburg2018nighres}, implementing cortical
reconstruction from brain images in four steps: skull stripping,
tissue classification, region extraction, and cortical
reconstruction. Each step read files produced by the previous step,
and wrote files that were or were not read by the subsequent step.
More information on this application is available in the Nighres
documentation at
\url{https://nighres.readthedocs.io/en/latest/auto_examples/example_02_cortical_depth_estimation.html}.
The application is implemented as a Python script that calls Java
image-processing routines. We used Python 3.6, Java 8, and Nighres
1.3.0. We patched the application to remove lazy data loading and
data compression, which made CPU time difficult to separate from
I/O time, and to capture task CPU times to inject them in the
simulation. The patched code is available at
\url{https://github.com/dohoangdzung/nighres}.
\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
    \toprule
        \multicolumn{1}{c}{Workflow step}& Input size       & Output size      & CPU time\\
                               & (MB)             & (MB)             & (s)\\
    \midrule
       Skull stripping         &  295             & 393               & 137 \\
       Tissue classification   &  197              & 1376              & 614 \\
       Region extraction       &  1376             & 885              & 76 \\
       Cortical reconstruction &  393              & 786              & 272\\
    \bottomrule
    \end{tabular} 
    \caption{Nighres application parameters}
    \label{table:nighres_stats}
    \end{table}
    \begin{table}[b]
        \centering
        \begin{tabularx}{\columnwidth}{ll
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X}
        \toprule
            \multicolumn{2}{c}{Bandwidths}  & Cluster (real) & Python prototype & \wrench simulator\\
        \midrule
        \multirow{2}{*}{Memory}      & read  & 6860 & 4812 & 4812\\
                                     & write & 2764 & 4812 & 4812\\
        \multirow{2}{*}{Local disk}  & read  & 510  & 465  & 465\\
                                     & write & 420  & 465  & 465\\
        \multirow{2}{*}{Remote disk} & read  & 515  & -    & 445\\
                                     & write & 375  & -    & 445\\
        \multicolumn{2}{l}{Network}  & 3000  & -    & 3000\\
        \bottomrule
    \end{tabularx}
    \caption{Bandwidth benchmarks (MBps) and simulator configurations.
    The bandwidths used in the simulations were the average of the measured read and write bandwidths.
    Network accesses were not simulated in the Python prototype.}
    \label{table:benchmark}
\end{table}

We used the real application in the fourth experiment
(\textit{Exp~4}), run on a single cluster node 
using a single local disk. We processed data from
participant 0027430 in the dataset of the Max Planck Institute for
Human Cognitive and Brain Sciences available at
\url{http://dx.doi.org/10.15387/fcp_indi.corr.mpg1}, leading to the
parameters in Table~\ref{table:nighres_stats}.


To parameterize the simulators, we benchmarked the
memory, local disk, remote disk (NFS), and network bandwidths
(Table~\ref{table:benchmark}). Since \simgrid, and thus \wrench, currently only supports
symmetrical bandwidths, we use the mean of the read and write
bandwidth values in our experiments.

% Finally, because \wrench simulates applications base on network-communication,
% we use an infinite bandwidth to eliminate network latency for local I/Os.